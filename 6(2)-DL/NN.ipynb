{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instructions:*\n",
    "- **과제 명세서를 읽어주시고 코드 작성을 해주시길 바랍니다**</span> \n",
    "- **명시된 step을 따라가며 전체적인 학습 방법을 숙지합니다**</span>\n",
    "- (**첫 번째 cell 결과로 나온 시간을 기준으로 채점을 하겠습니다**</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2025-01-26 19:57:35.394543\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Mutilayer Perceptron(```class MutiLayerPerceptron```)으로 간단한 Binary classification task를 진행해볼 것입니다. \n",
    "\n",
    "> 1. **Dataset**\n",
    ">> $\\texttt{moon}$ dataset\n",
    "> 2. **Network architecture**\n",
    "\n",
    " > $H_1 = X \\cdot W_1 + b_1$   \n",
    " > $z_1 = ReLU(H_1)$ where $ReLU$($=\\max(0,x)$) is a rectified linear unit and $z_1$ is an output of the first hidden layer.  \n",
    " > $H_2 = z_1 \\cdot W_2 + b_2$   \n",
    " > $z_2 = LeakyReLU(H_2)$ where $LeakyReLU$($=\\max(0.01x,x)$) and $z_2$ is an output of the second hidden layer. \n",
    " > $H_3 = z_2 \\cdot W_3 + b_3$   \n",
    " > $z_3 = tanh(H_3 + H_1)$ where $\\tanh$ is a tanh function and $z_3$ is an output of the third hidden layer.  \n",
    " > $H_4 = z_3 \\cdot W_4 + b_4$   \n",
    " > $\\hat y = \\sigma(H_4)$ where $\\sigma$ is a sigmoid function unit and $\\hat y$ is an output of the network.\n",
    " \n",
    " > **$W$** and **$b$**는 각각 weights와 bias.    \n",
    " > **weight 초기화**: Standard normal ($\\texttt{np.random.randn}$. 사용)  \n",
    " > **bias 초기화(intercept)**: 0     \n",
    " > **Input size**: 2  \n",
    " > **The first hidden layer size**: 10  \n",
    " > **The second hidden layer size**: 10  \n",
    " > **Output size**: 1   \n",
    " > **Regularization parameter $\\lambda$**: 0.001  \n",
    " > **Loss function**: Binary cross entropy loss (or equivently log loss).  \n",
    " > **Total loss** : \n",
    " > $L_{total} = \\sum_{i=1}^N{ (-y^{(i)}\\log \\hat{y}^{(i)} -(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) } +  \\lambda \\|W\\|^2 $   \n",
    " > **Optimization**: Gradient descent  \n",
    " > **Learning rate** = 0.0001  \n",
    " > **Number of epochs** = 50000  \n",
    " > $y$는 정답, $\\hat{y}$는 예측값이고 0부터 1사이에 존재한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from mlp import MultiLayerPerceptron\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Load data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2304d7cf920>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 1: Load data\")\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = sklearn.datasets.make_moons(300, noise = 0.25)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s = 40, c=y_train, cmap=plt.cm.RdYlGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Train the model\n",
      "Loss (epoch 1000): 45.83595769487307\n",
      "Loss (epoch 2000): 39.65089395093327\n",
      "Loss (epoch 3000): 38.11226349269628\n",
      "Loss (epoch 4000): 37.3452633533402\n",
      "Loss (epoch 5000): 36.79030736311145\n",
      "Loss (epoch 6000): 36.439940648277414\n",
      "Loss (epoch 7000): 35.580469124436526\n",
      "Loss (epoch 8000): 35.05451763433285\n",
      "Loss (epoch 9000): 34.73646270334263\n",
      "Loss (epoch 10000): 34.4394245771681\n",
      "Loss (epoch 11000): 34.184415556518594\n",
      "Loss (epoch 12000): 33.78047035704501\n",
      "Loss (epoch 13000): 33.32224892748088\n",
      "Loss (epoch 14000): 32.79346219972773\n",
      "Loss (epoch 15000): 32.57647793467644\n",
      "Loss (epoch 16000): 32.46872963694283\n",
      "Loss (epoch 17000): 32.07156971863446\n",
      "Loss (epoch 18000): 31.45721873994877\n",
      "Loss (epoch 19000): 31.108205075589584\n",
      "Loss (epoch 20000): 30.98479286528666\n",
      "Loss (epoch 21000): 30.920056601734117\n",
      "Loss (epoch 22000): 30.87312492055753\n",
      "Loss (epoch 23000): 30.89307566577991\n",
      "Loss (epoch 24000): 31.09774739652708\n",
      "Loss (epoch 25000): 30.909689731502322\n",
      "Loss (epoch 26000): 30.785635594238983\n",
      "Loss (epoch 27000): 30.79131883877993\n",
      "Loss (epoch 28000): 30.738994106195022\n",
      "Loss (epoch 29000): 30.68622567987582\n",
      "Loss (epoch 30000): 30.783337433801943\n",
      "Loss (epoch 31000): 30.52981059278389\n",
      "Loss (epoch 32000): 30.430017793728357\n",
      "Loss (epoch 33000): 30.405399120649516\n",
      "Loss (epoch 34000): 30.335319597389418\n",
      "Loss (epoch 35000): 30.292429223489783\n",
      "Loss (epoch 36000): 30.225315688422857\n",
      "Loss (epoch 37000): 30.110270932199324\n",
      "Loss (epoch 38000): 30.074200758695884\n",
      "Loss (epoch 39000): 30.016909281902926\n",
      "Loss (epoch 40000): 29.987442686318357\n",
      "Loss (epoch 41000): 29.927134535983797\n",
      "Loss (epoch 42000): 29.996498144908344\n",
      "Loss (epoch 43000): 29.815547173423774\n",
      "Loss (epoch 44000): 29.682592426860182\n",
      "Loss (epoch 45000): 29.64004224528703\n",
      "Loss (epoch 46000): 29.4636577911896\n",
      "Loss (epoch 47000): 29.34383635068967\n",
      "Loss (epoch 48000): 29.21948212756739\n",
      "Loss (epoch 49000): 29.026834756091386\n",
      "Loss (epoch 50000): 28.670271054678153\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: Train the model\")\n",
    "# random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "nn_input_dim = 2\n",
    "nn_output_dim = 1\n",
    "nn_hdim1 = 10\n",
    "nn_hdim2 = 10\n",
    "nn_hdim3 = 10\n",
    "lr = 0.0001 \n",
    "L2_norm = 0.001\n",
    "epoch = 50000\n",
    "\n",
    "model = MultiLayerPerceptron(nn_input_dim, nn_hdim1, nn_hdim2, nn_hdim3, nn_output_dim, init=\"random\")\n",
    "stats = model.train(X_train, y_train, learning_rate=lr, L2_norm=L2_norm, epoch=epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Plot decision boundary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Decision Boundary: Hidden layer dimension (10, 10)')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 3: Plot decision boundary\")\n",
    "# Plot the decision boundary\n",
    "utils.plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)\n",
    "plt.title(f\"Decision Boundary: Hidden layer dimension {nn_hdim1, nn_hdim2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(stats['loss_history'])) * 1000, stats['loss_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epoch')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(stats['train_acc_history'])) * 1000, stats['train_acc_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.title('Training accuracy over epoch')\n",
    "plt.gcf().set_size_inches(20, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
